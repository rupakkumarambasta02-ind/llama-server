# Local LLM API with CodeLlama 7B via Ollama

This project wraps the open-source CodeLlama 7B model (running locally via Ollama) in an OpenAI-compatible REST API. It supports both standard and streaming chat completions.

ğŸš€ Features
âœ… Local inference using CodeLlama 7B

âœ… OpenAI-style /v1/chat/completions endpoint

âœ… Streaming and non-streaming support

âœ… LangChain-compatible prompt templates

âœ… Easy integration with existing OpenAI clients

ğŸ› ï¸ Installation
1. Install Ollama
Download and install Ollama from https://ollama.com/download

Verify installation:

ollama --version

2. Pull CodeLlama 7B Model

ollama pull codellama:7b


ğŸ§ª Run the Model
Start the model server:

ollama run codellama:7b

This exposes a local endpoint at:

http://localhost:11434/api/chat

ğŸ§° Non-Streaming Setup
â–¶ï¸ Start the Non-Streaming Server

python server.py

This exposes:
POST http://localhost:8000/openai/v1/chat/completions

ğŸ’¬ Run the Non-Streaming Client
python client.py

The client sends OpenAI-style requests and prints the full response after completion.

ğŸŒŠ Streaming Setup

â–¶ï¸ Start the Streaming Server
python server_stream.py

This exposes:
POST http://localhost:8000/openai/v1/chat/completions

with stream: true support.

ğŸ’¬ Run the Streaming Client
python client_stream.py

The client sends a streaming request and prints tokens as they arrive.

ğŸ“„ Example Payload
{
  "model": "codellama:7b",
  "messages": [
    { "role": "user", "content": "Write a Python function to reverse a string." }
  ],
  "temperature": 0.7,
  "max_tokens": 500,
  "stream": true
}

ğŸ§© Notes
Ollama auto-detects GPU (e.g., RTX 3060) and uses it if available.

Streaming responses follow OpenAIâ€™s SSE format (data: {...}\n\n).

You can integrate this with LangChain using ChatOpenAI and base_url="http://localhost:8000/openai/v1".